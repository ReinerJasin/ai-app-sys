{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfYo3mh1+1YXnFVBChb/k4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReinerJasin/ai-app-sys/blob/main/Week13_12224827.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the TextVectorization layer"
      ],
      "metadata": {
        "id": "3-BDIIgCBgKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfmJbcXCBVK_"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "\n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in dataset:\n",
        "      text = self.standardize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "    \n",
        "    self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "  def encode(self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "  def decode(self, int_sequence):\n",
        "    return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLco_KKbCvil",
        "outputId": "2725a463-bafb-4060-d3e2-0375145659c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meXYLsIrED9y",
        "outputId": "c0a92866-2fe3-4e38-b651-f7cd41f04e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode = \"int\",\n",
        ")"
      ],
      "metadata": {
        "id": "AA9aOX_VEU2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "  lowercase_string = tf.strings.lower(string_tensor)\n",
        "  return tf.strings.regex_replace(lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "  return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode = \"int\",\n",
        "    standardize = custom_standardization_fn,\n",
        "    split = custom_split_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "x4EP6SMEEfER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "sFOAe2I1FWC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Displaying the vocabulary**"
      ],
      "metadata": {
        "id": "ac_9youZH7wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm69spFOHuOo",
        "outputId": "a8d6df93-badf-4da7-a764-f7c1f1995fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hra_QdOTIHHn",
        "outputId": "5e17833c-ce6b-42df-b98d-2d1244753554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two approaches for representing groups of words: Sets and sequences"
      ],
      "metadata": {
        "id": "Kl_gIQjGKMMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the IMDB movie reviews data"
      ],
      "metadata": {
        "id": "GKHIVQb3KRme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQbJCuKeKXCN",
        "outputId": "99ca8b1a-aa4c-4589-9353-25e9741a651d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  46.0M      0  0:00:01  0:00:01 --:--:-- 45.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "cXdoOOQNM-CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L842nUo7ND8-",
        "outputId": "600843db-634b-4bda-f8eb-3bee6bf89174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category)\n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname, val_dir / category / fname)"
      ],
      "metadata": {
        "id": "PjoyQpwfPepS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size = batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size = batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size = batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0pha26sgG9T",
        "outputId": "a74b0e55-9847-4ca5-a1cf-47a95558f2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Displaying the shapes and dtypes of the first batch**"
      ],
      "metadata": {
        "id": "kBB2UPDfgmJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]\", inputs[0])\n",
        "  print(\"targets[0]\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30BQPke6glpe",
        "outputId": "18ab97f3-10a5-494e-f026-18f8d66d84c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0] tf.Tensor(b'An Inconvenient Truth is as entirely simplistic and demagogic as the turgid slop created by the rabid and idiotic Republicans, it meanders along intangible lines until it attempts to gorge something into your face, namely that we\\'ll all be dead in a few hundred years, which is already indisputable, but who cares, humans are selfish, destructive creatures, I frankly do not waste my time caring about human extinction. I\\'ll just call it a \"natural progression\". Let the apocalypse begin, but meanwhile, we have to listen to the same brazen, slanted politicians who propose another \"new society\", well, don\\'t be fooled, we\\'ll all still be controlled by the wealthy, by those in power and by those idiots who created the catastrophes in the first place. Nothing will ever change.<br /><br />Al Gore, whose hypocrisy is quite evident in the film, as he is being driven in a gas guzzling car all alone using a consumerist computer, he also lives on huge acres of land in a rather large mansion, the land itself was used for destructive erosive purposes including cattle, tobacco, pig farming (which accounts for methane gas traces) and who knows what else, his wealth is predicated on exploitation, greed and his investments include numerous large companies in the world with disputable records. I hardly think this man is qualified to lecture the less fortunate, but his prestige is based on his opposition to another ludicrous political party, that is all, meanwhile he emits those very same rancid characteristics that make politics and politicians so appalling. This bozo happens to be living the comfortable life and yet he\\'s lecturing poor people in Africa about crop farming and cut and burn techniques? He travels across the world in first class seats in fuel wasting jets, uses product placed computers in the documentary, and yet he thinks everything is a \"moral issue\". He\\'s entirely absorbed in his own deluded nightmares, he says he came to these conclusions because of the death of his sister (from tobacco induced cancer and the near death of his son by an automobile of all things). Did he fight against the tobacco companies or propose that automobiles be banned because they are dangerous hulking machines? NO. Everything must serve the \"economy\", so why is he any different, the answer is he is not.<br /><br />His forlorn and exhausted attempts at humanistic philosophy are disastrous, all this while he\\'s being filmed in the forest or along a little river eschewing stale life affirming quotes. Well Mr Gore, why don\\'t you try living like the common people then? He is a politician, plain and simple, he has a career invested in the power structure. My question is, why doesn\\'t he concentrate on the powerful industrial nations of the earth who are to blame for most of the complications? He doesn\\'t do that because it would be unwise for \"investments, stocks and corporations\".<br /><br />Al Gore gives monotonous lectures about the subject in the documentary, namely to wealthy white people in the audience, who clap on cue, while showing them graph charts, numbers and percentages, and speaking in a dreary tone, no one without a Harvard (which the elites control) education can make sense out of it, but he tells us everything is going to hell. No kidding, but I think he fails to account for this problem precisely in the approach that capitalism has taken for the planet, namely that it is expendable and a waste dump. He never once mentions how industrialization has created these problems, he just wants to put mild bandages on them but not eradicate the whole oppressive system. Its obvious he was spoiled, sent to the schools for elites and has the same basic temperament for politics as any other back stabbing, inconsistent dullard in Washington. Whoever made this propaganda, as it is in no way different than what the Republicans have conceived, had only goals in mind that were directed by capitalistic impulse. That is to say, someone is going to benefit, and it seems the \"new green\" politicians who support venture capitalist companies who are buying up hordes of land in an attempt to develop the \"new Utopian future\" with \"new technologies\". It\\'s the same old story, Al Gore is a believer in the elitist structure, he actually believes there is a \"democracy\" in the US which I find very naive. If we aren\\'t paying wages to the oil companies, then we\\'ll be paying them to the wind and solar companies.<br /><br />I find the speech at the end quite rancid, along the lines of something GW Bush would have oozed over to the dumb downed masses, Gore speaks about \"people uniting together to defeat communism\" in the 1990\\'s, what it had to do with global warming, absolutely nothing but he attempts to get base emotions ruminating in people. With that said, he didn\\'t understand that communism never existed in the world, the systems in Europe and USSR were merely a tyrannical form of authoritarianism and capitalism, no less different than what controls the US interests. Social ecology was not even mentioned here, which is really a travesty. If you want to change the world, then one must dispose of those antiquated systems that are based on greed, exploitation and violence.', shape=(), dtype=string)\n",
            "targets[0] tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing words as a set: The bag-of-words approach"
      ],
      "metadata": {
        "id": "Ntyx_LO4jK2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single words (unigrams) with binary encoding"
      ],
      "metadata": {
        "id": "8SeQ51TAjQOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing our datasets with a** ```TextVectorization``` **layer**\n",
        "\n"
      ],
      "metadata": {
        "id": "XmY4fYwQjsU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"multi_hot\",\n",
        ")\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        ")\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        ")\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        ")"
      ],
      "metadata": {
        "id": "1Kd0i-D0jP5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ],
      "metadata": {
        "id": "vBGHltFBmYgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]\", inputs[0])\n",
        "  print(\"targets[0]\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSej8LkQmdoG",
        "outputId": "f0ec8968-4741-4bad-cb85-f7c6eba6eafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0] tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0] tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our model-building utility**"
      ],
      "metadata": {
        "id": "THDvTVMunbZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens = 20000, hidden_dim = 16):\n",
        "  inputs = keras.Input(shape = (max_tokens,))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\",\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "befBpN8rng_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing the binary unigram model**"
      ],
      "metadata": {
        "id": "MyVVKVp2pT3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only = True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data = binary_1gram_val_ds.cache(),\n",
        "          epochs = 10,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opszT4zupYCK",
        "outputId": "62172d86-256c-4257-b41b-07573e955572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 11s 14ms/step - loss: 0.3968 - accuracy: 0.8345 - val_loss: 0.3158 - val_accuracy: 0.8722\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2681 - accuracy: 0.9025 - val_loss: 0.3097 - val_accuracy: 0.8844\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2378 - accuracy: 0.9176 - val_loss: 0.3273 - val_accuracy: 0.8866\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2219 - accuracy: 0.9247 - val_loss: 0.3448 - val_accuracy: 0.8856\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2175 - accuracy: 0.9301 - val_loss: 0.3555 - val_accuracy: 0.8850\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2080 - accuracy: 0.9349 - val_loss: 0.3773 - val_accuracy: 0.8844\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2109 - accuracy: 0.9351 - val_loss: 0.3805 - val_accuracy: 0.8862\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2084 - accuracy: 0.9383 - val_loss: 0.3911 - val_accuracy: 0.8840\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2000 - accuracy: 0.9376 - val_loss: 0.4019 - val_accuracy: 0.8840\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2061 - accuracy: 0.9357 - val_loss: 0.4057 - val_accuracy: 0.8818\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.2943 - accuracy: 0.8885\n",
            "Test acc: 0.888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigrams with binary encoding"
      ],
      "metadata": {
        "id": "OMw8yhAGqD05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuring the** ```TextVectorization``` **layer to return bigrams**"
      ],
      "metadata": {
        "id": "JpX3uv9RqI09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"multi_hot\",\n",
        ")"
      ],
      "metadata": {
        "id": "IPJ21phmqII0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing the binary bigram model**"
      ],
      "metadata": {
        "id": "AQl67EBVrGL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        "    )\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        "    )\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        "    )\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data = binary_2gram_val_ds.cache(),\n",
        "          epochs = 10,\n",
        "          callbacks = callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kDg4J4orKnp",
        "outputId": "746283aa-83ad-4cd1-8639-a5beac8a900e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 13ms/step - loss: 0.3909 - accuracy: 0.8371 - val_loss: 0.2794 - val_accuracy: 0.8900\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2425 - accuracy: 0.9123 - val_loss: 0.2859 - val_accuracy: 0.8934\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2036 - accuracy: 0.9319 - val_loss: 0.3000 - val_accuracy: 0.8962\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1968 - accuracy: 0.9377 - val_loss: 0.3119 - val_accuracy: 0.8918\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1822 - accuracy: 0.9438 - val_loss: 0.3294 - val_accuracy: 0.8926\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1756 - accuracy: 0.9459 - val_loss: 0.3366 - val_accuracy: 0.8894\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1739 - accuracy: 0.9502 - val_loss: 0.3654 - val_accuracy: 0.8860\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1763 - accuracy: 0.9516 - val_loss: 0.3571 - val_accuracy: 0.8902\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1728 - accuracy: 0.9530 - val_loss: 0.3565 - val_accuracy: 0.8866\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1655 - accuracy: 0.9516 - val_loss: 0.3731 - val_accuracy: 0.8842\n",
            "782/782 [==============================] - 11s 13ms/step - loss: 0.2683 - accuracy: 0.8958\n",
            "Test acc: 0.896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigrams with TF-IDF encoding"
      ],
      "metadata": {
        "id": "AVEo4PGpsftX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuring the** ```TextVectorization``` **layer to return token counts**"
      ],
      "metadata": {
        "id": "Efh5DpQxsjB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"count\",\n",
        ")"
      ],
      "metadata": {
        "id": "4Y9VMb75s02_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuring** ```TextVectorization``` **to return TF-IDF-weighted outputs**"
      ],
      "metadata": {
        "id": "G-eQijzos0nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"tf_idf\",\n",
        ")"
      ],
      "metadata": {
        "id": "mGOTH70dtCZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ],
      "metadata": {
        "id": "OVqZcpSqthTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\", save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data = tfidf_2gram_val_ds.cache(),\n",
        "          epochs = 10,\n",
        "          callbacks = callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61YeC1AVtlsS",
        "outputId": "4289ab9b-fb41-4f68-955d-6a3d0ff3c516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 20ms/step - loss: 0.4937 - accuracy: 0.7758 - val_loss: 0.3358 - val_accuracy: 0.8574\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3364 - accuracy: 0.8597 - val_loss: 0.2932 - val_accuracy: 0.8882\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3100 - accuracy: 0.8728 - val_loss: 0.3451 - val_accuracy: 0.8504\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2847 - accuracy: 0.8820 - val_loss: 0.3107 - val_accuracy: 0.8678\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2670 - accuracy: 0.8905 - val_loss: 0.3272 - val_accuracy: 0.8614\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2509 - accuracy: 0.8997 - val_loss: 0.3857 - val_accuracy: 0.8440\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2401 - accuracy: 0.8998 - val_loss: 0.3442 - val_accuracy: 0.8544\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2419 - accuracy: 0.9007 - val_loss: 0.3585 - val_accuracy: 0.8484\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2269 - accuracy: 0.9024 - val_loss: 0.3451 - val_accuracy: 0.8676\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2277 - accuracy: 0.9019 - val_loss: 0.3477 - val_accuracy: 0.8590\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.2997 - accuracy: 0.8907\n",
            "Test acc: 0.891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape = (1,), dtype = \"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "2-TjD7YKuXd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkbbcLpHuwnr",
        "outputId": "5f67a3f8-36bb-4770-be46-8b05546fdad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95.72 percent positive\n"
          ]
        }
      ]
    }
  ]
}